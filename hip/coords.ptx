//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-26907403
// Cuda compilation tools, release 10.1, V10.1.243
// Based on LLVM 3.4svn
//

.version 6.4
.target sm_30
.address_size 64

	// .globl	_Z9cs_cartszP8coords_sPi
.const .align 8 .b8 __nv_static_27__14_coords_cpp1_ii_7126c51d_const_param[120];

.visible .func  (.param .b32 func_retval0) _Z9cs_cartszP8coords_sPi(
	.param .b64 _Z9cs_cartszP8coords_sPi_param_0,
	.param .b64 _Z9cs_cartszP8coords_sPi_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [_Z9cs_cartszP8coords_sPi_param_0];
	ld.param.u64 	%rd2, [_Z9cs_cartszP8coords_sPi_param_1];
	setp.ne.s64	%p1, %rd1, 0;
	@%p1 bra 	BB0_2;

	// inline asm
	trap;
	// inline asm

BB0_2:
	ld.u64 	%rd3, [%rd1+16];
	ld.u32 	%r1, [%rd3+68];
	st.u32 	[%rd2], %r1;
	ld.u64 	%rd4, [%rd1+16];
	ld.u32 	%r2, [%rd4+72];
	st.u32 	[%rd2+4], %r2;
	ld.u64 	%rd5, [%rd1+16];
	ld.u32 	%r3, [%rd5+76];
	st.u32 	[%rd2+8], %r3;
	mov.u32 	%r4, 0;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	// .globl	_Z14cs_cart_coordsP8coords_sPi
.visible .func  (.param .b32 func_retval0) _Z14cs_cart_coordsP8coords_sPi(
	.param .b64 _Z14cs_cart_coordsP8coords_sPi_param_0,
	.param .b64 _Z14cs_cart_coordsP8coords_sPi_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [_Z14cs_cart_coordsP8coords_sPi_param_0];
	ld.param.u64 	%rd2, [_Z14cs_cart_coordsP8coords_sPi_param_1];
	setp.ne.s64	%p1, %rd1, 0;
	@%p1 bra 	BB1_2;

	// inline asm
	trap;
	// inline asm

BB1_2:
	ld.u64 	%rd3, [%rd1+16];
	ld.u32 	%r1, [%rd3+80];
	st.u32 	[%rd2], %r1;
	ld.u64 	%rd4, [%rd1+16];
	ld.u32 	%r2, [%rd4+84];
	st.u32 	[%rd2+4], %r2;
	ld.u64 	%rd5, [%rd1+16];
	ld.u32 	%r3, [%rd5+88];
	st.u32 	[%rd2+8], %r3;
	mov.u32 	%r4, 0;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	// .globl	_Z11cs_periodicP8coords_sPi
.visible .func  (.param .b32 func_retval0) _Z11cs_periodicP8coords_sPi(
	.param .b64 _Z11cs_periodicP8coords_sPi_param_0,
	.param .b64 _Z11cs_periodicP8coords_sPi_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [_Z11cs_periodicP8coords_sPi_param_0];
	ld.param.u64 	%rd2, [_Z11cs_periodicP8coords_sPi_param_1];
	setp.ne.s64	%p1, %rd1, 0;
	@%p1 bra 	BB2_2;

	// inline asm
	trap;
	// inline asm

BB2_2:
	ld.u64 	%rd3, [%rd1+16];
	ld.u32 	%r1, [%rd3+56];
	st.u32 	[%rd2], %r1;
	ld.u64 	%rd4, [%rd1+16];
	ld.u32 	%r2, [%rd4+60];
	st.u32 	[%rd2+4], %r2;
	ld.u64 	%rd5, [%rd1+16];
	ld.u32 	%r3, [%rd5+64];
	st.u32 	[%rd2+8], %r3;
	mov.u32 	%r4, 0;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	// .globl	_Z7cs_ltotP8coords_sPd
.visible .func  (.param .b32 func_retval0) _Z7cs_ltotP8coords_sPd(
	.param .b64 _Z7cs_ltotP8coords_sPd_param_0,
	.param .b64 _Z7cs_ltotP8coords_sPd_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [_Z7cs_ltotP8coords_sPd_param_0];
	ld.param.u64 	%rd2, [_Z7cs_ltotP8coords_sPd_param_1];
	setp.ne.s64	%p1, %rd1, 0;
	@%p1 bra 	BB3_2;

	// inline asm
	trap;
	// inline asm

BB3_2:
	ld.u64 	%rd3, [%rd1+16];
	ld.u32 	%r1, [%rd3+8];
	cvt.rn.f64.s32	%fd1, %r1;
	st.f64 	[%rd2], %fd1;
	ld.u64 	%rd4, [%rd1+16];
	ld.u32 	%r2, [%rd4+12];
	cvt.rn.f64.s32	%fd2, %r2;
	st.f64 	[%rd2+8], %fd2;
	ld.u64 	%rd5, [%rd1+16];
	ld.u32 	%r3, [%rd5+16];
	cvt.rn.f64.s32	%fd3, %r3;
	st.f64 	[%rd2+16], %fd3;
	mov.u32 	%r4, 0;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	// .globl	_Z7cs_lminP8coords_sPd
.visible .func  (.param .b32 func_retval0) _Z7cs_lminP8coords_sPd(
	.param .b64 _Z7cs_lminP8coords_sPd_param_0,
	.param .b64 _Z7cs_lminP8coords_sPd_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<2>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [_Z7cs_lminP8coords_sPd_param_0];
	ld.param.u64 	%rd2, [_Z7cs_lminP8coords_sPd_param_1];
	setp.ne.s64	%p1, %rd1, 0;
	@%p1 bra 	BB4_2;

	// inline asm
	trap;
	// inline asm

BB4_2:
	ld.u64 	%rd3, [%rd1+16];
	ld.f64 	%fd1, [%rd3+96];
	st.f64 	[%rd2], %fd1;
	ld.u64 	%rd4, [%rd1+16];
	ld.f64 	%fd2, [%rd4+104];
	st.f64 	[%rd2+8], %fd2;
	ld.u64 	%rd5, [%rd1+16];
	ld.f64 	%fd3, [%rd5+112];
	st.f64 	[%rd2+16], %fd3;
	mov.u32 	%r1, 0;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .globl	_Z9cs_nlocalP8coords_sPi
.visible .func  (.param .b32 func_retval0) _Z9cs_nlocalP8coords_sPi(
	.param .b64 _Z9cs_nlocalP8coords_sPi_param_0,
	.param .b64 _Z9cs_nlocalP8coords_sPi_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [_Z9cs_nlocalP8coords_sPi_param_0];
	ld.param.u64 	%rd2, [_Z9cs_nlocalP8coords_sPi_param_1];
	setp.ne.s64	%p1, %rd1, 0;
	@%p1 bra 	BB5_2;

	// inline asm
	trap;
	// inline asm

BB5_2:
	ld.u64 	%rd3, [%rd1+16];
	ld.u32 	%r1, [%rd3+20];
	st.u32 	[%rd2], %r1;
	ld.u64 	%rd4, [%rd1+16];
	ld.u32 	%r2, [%rd4+24];
	st.u32 	[%rd2+4], %r2;
	ld.u64 	%rd5, [%rd1+16];
	ld.u32 	%r3, [%rd5+28];
	st.u32 	[%rd2+8], %r3;
	mov.u32 	%r4, 0;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	// .globl	_Z9cs_nsitesP8coords_sPi
.visible .func  (.param .b32 func_retval0) _Z9cs_nsitesP8coords_sPi(
	.param .b64 _Z9cs_nsitesP8coords_sPi_param_0,
	.param .b64 _Z9cs_nsitesP8coords_sPi_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z9cs_nsitesP8coords_sPi_param_0];
	ld.param.u64 	%rd2, [_Z9cs_nsitesP8coords_sPi_param_1];
	setp.ne.s64	%p1, %rd1, 0;
	@%p1 bra 	BB6_2;

	// inline asm
	trap;
	// inline asm

BB6_2:
	ld.u64 	%rd3, [%rd1+16];
	ld.u32 	%r1, [%rd3+4];
	st.u32 	[%rd2], %r1;
	mov.u32 	%r2, 0;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

	// .globl	_Z16cs_nlocal_offsetP8coords_sPi
.visible .func  (.param .b32 func_retval0) _Z16cs_nlocal_offsetP8coords_sPi(
	.param .b64 _Z16cs_nlocal_offsetP8coords_sPi_param_0,
	.param .b64 _Z16cs_nlocal_offsetP8coords_sPi_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [_Z16cs_nlocal_offsetP8coords_sPi_param_0];
	ld.param.u64 	%rd2, [_Z16cs_nlocal_offsetP8coords_sPi_param_1];
	setp.ne.s64	%p1, %rd1, 0;
	@%p1 bra 	BB7_2;

	// inline asm
	trap;
	// inline asm

BB7_2:
	ld.u64 	%rd3, [%rd1+16];
	ld.u32 	%r1, [%rd3+32];
	st.u32 	[%rd2], %r1;
	ld.u64 	%rd4, [%rd1+16];
	ld.u32 	%r2, [%rd4+36];
	st.u32 	[%rd2+4], %r2;
	ld.u64 	%rd5, [%rd1+16];
	ld.u32 	%r3, [%rd5+40];
	st.u32 	[%rd2+8], %r3;
	mov.u32 	%r4, 0;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	// .globl	_Z8cs_indexP8coords_siii
.visible .func  (.param .b32 func_retval0) _Z8cs_indexP8coords_siii(
	.param .b64 _Z8cs_indexP8coords_siii_param_0,
	.param .b32 _Z8cs_indexP8coords_siii_param_1,
	.param .b32 _Z8cs_indexP8coords_siii_param_2,
	.param .b32 _Z8cs_indexP8coords_siii_param_3
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<51>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd14, [_Z8cs_indexP8coords_siii_param_0];
	ld.param.u32 	%r16, [_Z8cs_indexP8coords_siii_param_1];
	ld.param.u32 	%r17, [_Z8cs_indexP8coords_siii_param_2];
	ld.param.u32 	%r18, [_Z8cs_indexP8coords_siii_param_3];
	setp.ne.s64	%p1, %rd14, 0;
	@%p1 bra 	BB8_2;

	// inline asm
	trap;
	// inline asm

BB8_2:
	setp.eq.s32	%p2, %r16, 0;
	selp.u32	%r19, 1, 0, %p2;
	ld.u64 	%rd16, [%rd14+16];
	ld.u32 	%r46, [%rd16];
	mov.u32 	%r20, 1;
	sub.s32 	%r21, %r20, %r46;
	setp.lt.s32	%p3, %r19, %r21;
	@%p3 bra 	BB8_4;

	// inline asm
	trap;
	// inline asm
	ld.u64 	%rd16, [%rd14+16];
	ld.u32 	%r46, [%rd16];

BB8_4:
	setp.eq.s32	%p4, %r17, 0;
	selp.u32	%r4, 1, 0, %p4;
	sub.s32 	%r23, %r20, %r46;
	setp.lt.s32	%p5, %r4, %r23;
	@%p5 bra 	BB8_6;

	// inline asm
	trap;
	// inline asm
	ld.u64 	%rd16, [%rd14+16];
	ld.u32 	%r46, [%rd16];

BB8_6:
	setp.eq.s32	%p6, %r18, 0;
	selp.u32	%r7, 1, 0, %p6;
	sub.s32 	%r25, %r20, %r46;
	setp.lt.s32	%p7, %r7, %r25;
	@%p7 bra 	BB8_8;

	// inline asm
	trap;
	// inline asm
	ld.u64 	%rd16, [%rd14+16];
	ld.u32 	%r46, [%rd16];

BB8_8:
	ld.u32 	%r26, [%rd16+20];
	add.s32 	%r27, %r46, %r26;
	setp.gt.s32	%p9, %r19, %r27;
	@%p9 bra 	BB8_10;

	// inline asm
	trap;
	// inline asm
	ld.u64 	%rd16, [%rd14+16];
	ld.u32 	%r46, [%rd16];

BB8_10:
	ld.u32 	%r29, [%rd16+24];
	add.s32 	%r30, %r46, %r29;
	setp.gt.s32	%p10, %r4, %r30;
	@%p10 bra 	BB8_12;

	// inline asm
	trap;
	// inline asm
	ld.u64 	%rd16, [%rd14+16];
	ld.u32 	%r46, [%rd16];

BB8_12:
	ld.u32 	%r31, [%rd16+28];
	add.s32 	%r32, %r46, %r31;
	setp.gt.s32	%p11, %r7, %r32;
	@%p11 bra 	BB8_14;

	// inline asm
	trap;
	// inline asm
	ld.u64 	%rd16, [%rd14+16];
	ld.u32 	%r46, [%rd16];

BB8_14:
	add.s32 	%r33, %r16, %r46;
	add.s32 	%r34, %r33, -1;
	ld.u32 	%r35, [%rd16+44];
	mul.lo.s32 	%r36, %r34, %r35;
	add.s32 	%r37, %r17, %r46;
	add.s32 	%r38, %r37, -1;
	ld.u32 	%r39, [%rd16+48];
	mad.lo.s32 	%r40, %r38, %r39, %r36;
	add.s32 	%r41, %r18, %r46;
	add.s32 	%r42, %r41, -1;
	ld.u32 	%r43, [%rd16+52];
	mad.lo.s32 	%r44, %r42, %r43, %r40;
	st.param.b32	[func_retval0+0], %r44;
	ret;
}

	// .globl	_Z8cs_nhaloP8coords_sPi
.visible .func  (.param .b32 func_retval0) _Z8cs_nhaloP8coords_sPi(
	.param .b64 _Z8cs_nhaloP8coords_sPi_param_0,
	.param .b64 _Z8cs_nhaloP8coords_sPi_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z8cs_nhaloP8coords_sPi_param_0];
	ld.param.u64 	%rd2, [_Z8cs_nhaloP8coords_sPi_param_1];
	setp.ne.s64	%p1, %rd1, 0;
	@%p1 bra 	BB9_2;

	// inline asm
	trap;
	// inline asm

BB9_2:
	setp.ne.s64	%p2, %rd2, 0;
	@%p2 bra 	BB9_4;

	// inline asm
	trap;
	// inline asm

BB9_4:
	ld.u64 	%rd3, [%rd1+16];
	ld.u32 	%r1, [%rd3];
	st.u32 	[%rd2], %r1;
	mov.u32 	%r2, 0;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

	// .globl	_Z9cs_ntotalP8coords_sPi
.visible .func  (.param .b32 func_retval0) _Z9cs_ntotalP8coords_sPi(
	.param .b64 _Z9cs_ntotalP8coords_sPi_param_0,
	.param .b64 _Z9cs_ntotalP8coords_sPi_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [_Z9cs_ntotalP8coords_sPi_param_0];
	ld.param.u64 	%rd2, [_Z9cs_ntotalP8coords_sPi_param_1];
	setp.ne.s64	%p1, %rd1, 0;
	@%p1 bra 	BB10_2;

	// inline asm
	trap;
	// inline asm

BB10_2:
	ld.u64 	%rd3, [%rd1+16];
	ld.u32 	%r1, [%rd3+8];
	st.u32 	[%rd2], %r1;
	ld.u64 	%rd4, [%rd1+16];
	ld.u32 	%r2, [%rd4+12];
	st.u32 	[%rd2+4], %r2;
	ld.u64 	%rd5, [%rd1+16];
	ld.u32 	%r3, [%rd5+16];
	st.u32 	[%rd2+8], %r3;
	mov.u32 	%r4, 0;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	// .globl	_Z19cs_minimum_distanceP8coords_sPKdS2_Pd
.visible .func  (.param .b32 func_retval0) _Z19cs_minimum_distanceP8coords_sPKdS2_Pd(
	.param .b64 _Z19cs_minimum_distanceP8coords_sPKdS2_Pd_param_0,
	.param .b64 _Z19cs_minimum_distanceP8coords_sPKdS2_Pd_param_1,
	.param .b64 _Z19cs_minimum_distanceP8coords_sPKdS2_Pd_param_2,
	.param .b64 _Z19cs_minimum_distanceP8coords_sPKdS2_Pd_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<43>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd16, [_Z19cs_minimum_distanceP8coords_sPKdS2_Pd_param_0];
	ld.param.u64 	%rd17, [_Z19cs_minimum_distanceP8coords_sPKdS2_Pd_param_1];
	ld.param.u64 	%rd18, [_Z19cs_minimum_distanceP8coords_sPKdS2_Pd_param_2];
	ld.param.u64 	%rd19, [_Z19cs_minimum_distanceP8coords_sPKdS2_Pd_param_3];
	setp.ne.s64	%p1, %rd16, 0;
	@%p1 bra 	BB11_2;

	// inline asm
	trap;
	// inline asm

BB11_2:
	ld.f64 	%fd16, [%rd18];
	ld.f64 	%fd17, [%rd17];
	sub.f64 	%fd40, %fd16, %fd17;
	st.f64 	[%rd19], %fd40;
	ld.f64 	%fd18, [%rd17+8];
	ld.f64 	%fd19, [%rd18+8];
	sub.f64 	%fd41, %fd19, %fd18;
	st.f64 	[%rd19+8], %fd41;
	ld.f64 	%fd20, [%rd17+16];
	ld.f64 	%fd21, [%rd18+16];
	sub.f64 	%fd42, %fd21, %fd20;
	st.f64 	[%rd19+16], %fd42;
	add.s64 	%rd1, %rd16, 16;
	ld.u64 	%rd20, [%rd16+16];
	add.s64 	%rd3, %rd20, 8;
	ld.u32 	%r17, [%rd20+8];
	cvt.rn.f64.s32	%fd4, %r17;
	mul.f64 	%fd22, %fd4, 0d3FE0000000000000;
	setp.leu.f64	%p2, %fd40, %fd22;
	@%p2 bra 	BB11_4;

	ld.u32 	%r10, [%rd3+48];
	cvt.rn.f64.s32	%fd23, %r10;
	mul.f64 	%fd24, %fd4, %fd23;
	sub.f64 	%fd40, %fd40, %fd24;
	st.f64 	[%rd19], %fd40;
	ld.u64 	%rd20, [%rd1];
	ld.u32 	%r17, [%rd20+8];

BB11_4:
	cvt.rn.f64.s32	%fd7, %r17;
	mul.f64 	%fd25, %fd7, 0dBFE0000000000000;
	setp.geu.f64	%p3, %fd40, %fd25;
	@%p3 bra 	BB11_6;

	ld.u32 	%r11, [%rd20+56];
	cvt.rn.f64.s32	%fd26, %r11;
	fma.rn.f64 	%fd27, %fd7, %fd26, %fd40;
	st.f64 	[%rd19], %fd27;
	ld.u64 	%rd20, [%rd1];

BB11_6:
	add.s64 	%rd8, %rd20, 12;
	ld.u32 	%r18, [%rd20+12];
	cvt.rn.f64.s32	%fd8, %r18;
	mul.f64 	%fd28, %fd8, 0d3FE0000000000000;
	setp.leu.f64	%p4, %fd41, %fd28;
	@%p4 bra 	BB11_8;

	ld.u32 	%r12, [%rd8+48];
	cvt.rn.f64.s32	%fd29, %r12;
	mul.f64 	%fd30, %fd8, %fd29;
	sub.f64 	%fd41, %fd41, %fd30;
	st.f64 	[%rd19+8], %fd41;
	ld.u64 	%rd20, [%rd1];
	ld.u32 	%r18, [%rd20+12];

BB11_8:
	cvt.rn.f64.s32	%fd11, %r18;
	mul.f64 	%fd31, %fd11, 0dBFE0000000000000;
	setp.geu.f64	%p5, %fd41, %fd31;
	@%p5 bra 	BB11_10;

	ld.u32 	%r13, [%rd20+60];
	cvt.rn.f64.s32	%fd32, %r13;
	fma.rn.f64 	%fd33, %fd11, %fd32, %fd41;
	st.f64 	[%rd19+8], %fd33;
	ld.u64 	%rd20, [%rd1];

BB11_10:
	add.s64 	%rd13, %rd20, 16;
	ld.u32 	%r19, [%rd20+16];
	cvt.rn.f64.s32	%fd12, %r19;
	mul.f64 	%fd34, %fd12, 0d3FE0000000000000;
	setp.leu.f64	%p6, %fd42, %fd34;
	@%p6 bra 	BB11_12;

	ld.u32 	%r14, [%rd13+48];
	cvt.rn.f64.s32	%fd35, %r14;
	mul.f64 	%fd36, %fd12, %fd35;
	sub.f64 	%fd42, %fd42, %fd36;
	st.f64 	[%rd19+16], %fd42;
	ld.u64 	%rd20, [%rd1];
	ld.u32 	%r19, [%rd20+16];

BB11_12:
	cvt.rn.f64.s32	%fd15, %r19;
	mul.f64 	%fd37, %fd15, 0dBFE0000000000000;
	setp.geu.f64	%p7, %fd42, %fd37;
	@%p7 bra 	BB11_14;

	ld.u32 	%r15, [%rd20+64];
	cvt.rn.f64.s32	%fd38, %r15;
	fma.rn.f64 	%fd39, %fd15, %fd38, %fd42;
	st.f64 	[%rd19+16], %fd39;

BB11_14:
	mov.u32 	%r16, 0;
	st.param.b32	[func_retval0+0], %r16;
	ret;
}

	// .globl	_Z15cs_index_to_ijkP8coords_siPi
.visible .func  (.param .b32 func_retval0) _Z15cs_index_to_ijkP8coords_siPi(
	.param .b64 _Z15cs_index_to_ijkP8coords_siPi_param_0,
	.param .b32 _Z15cs_index_to_ijkP8coords_siPi_param_1,
	.param .b64 _Z15cs_index_to_ijkP8coords_siPi_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd15, [_Z15cs_index_to_ijkP8coords_siPi_param_0];
	ld.param.u32 	%r20, [_Z15cs_index_to_ijkP8coords_siPi_param_1];
	ld.param.u64 	%rd16, [_Z15cs_index_to_ijkP8coords_siPi_param_2];
	setp.ne.s64	%p1, %rd15, 0;
	@%p1 bra 	BB12_2;

	// inline asm
	trap;
	// inline asm

BB12_2:
	add.s64 	%rd1, %rd15, 16;
	ld.u64 	%rd17, [%rd15+16];
	ld.u32 	%r21, [%rd17];
	mov.u32 	%r22, 1;
	sub.s32 	%r23, %r22, %r21;
	ld.u32 	%r24, [%rd17+44];
	div.s32 	%r25, %r20, %r24;
	add.s32 	%r1, %r23, %r25;
	st.u32 	[%rd16], %r1;
	ld.u64 	%rd18, [%rd15+16];
	ld.u32 	%r26, [%rd18];
	sub.s32 	%r27, %r22, %r26;
	ld.u32 	%r28, [%rd18+44];
	rem.s32 	%r29, %r20, %r28;
	ld.u32 	%r30, [%rd18+48];
	div.s32 	%r31, %r29, %r30;
	add.s32 	%r2, %r27, %r31;
	st.u32 	[%rd16+4], %r2;
	ld.u64 	%rd19, [%rd15+16];
	ld.u32 	%r32, [%rd19];
	sub.s32 	%r33, %r22, %r32;
	ld.u32 	%r34, [%rd19+48];
	rem.s32 	%r35, %r20, %r34;
	add.s32 	%r3, %r35, %r33;
	st.u32 	[%rd16+8], %r3;
	@%p1 bra 	BB12_4;

	// inline asm
	trap;
	// inline asm

BB12_4:
	setp.eq.s32	%p3, %r1, 0;
	selp.u32	%r4, 1, 0, %p3;
	ld.u64 	%rd21, [%rd1];
	ld.u32 	%r64, [%rd21];
	sub.s32 	%r37, %r22, %r64;
	setp.lt.s32	%p4, %r4, %r37;
	@%p4 bra 	BB12_6;

	// inline asm
	trap;
	// inline asm
	ld.u64 	%rd21, [%rd15+16];
	ld.u32 	%r64, [%rd21];

BB12_6:
	setp.eq.s32	%p5, %r2, 0;
	selp.u32	%r8, 1, 0, %p5;
	sub.s32 	%r39, %r22, %r64;
	setp.lt.s32	%p6, %r8, %r39;
	@%p6 bra 	BB12_8;

	// inline asm
	trap;
	// inline asm
	ld.u64 	%rd21, [%rd15+16];
	ld.u32 	%r64, [%rd21];

BB12_8:
	setp.eq.s32	%p7, %r3, 0;
	selp.u32	%r11, 1, 0, %p7;
	sub.s32 	%r41, %r22, %r64;
	setp.lt.s32	%p8, %r11, %r41;
	@%p8 bra 	BB12_10;

	// inline asm
	trap;
	// inline asm
	ld.u64 	%rd21, [%rd15+16];
	ld.u32 	%r64, [%rd21];

BB12_10:
	ld.u32 	%r42, [%rd21+20];
	add.s32 	%r43, %r64, %r42;
	setp.gt.s32	%p9, %r4, %r43;
	@%p9 bra 	BB12_12;

	// inline asm
	trap;
	// inline asm
	ld.u64 	%rd21, [%rd15+16];
	ld.u32 	%r64, [%rd21];

BB12_12:
	ld.u32 	%r44, [%rd21+24];
	add.s32 	%r45, %r64, %r44;
	setp.gt.s32	%p10, %r8, %r45;
	@%p10 bra 	BB12_14;

	// inline asm
	trap;
	// inline asm
	ld.u64 	%rd21, [%rd15+16];
	ld.u32 	%r64, [%rd21];

BB12_14:
	ld.u32 	%r46, [%rd21+28];
	add.s32 	%r47, %r64, %r46;
	setp.gt.s32	%p11, %r11, %r47;
	@%p11 bra 	BB12_16;

	// inline asm
	trap;
	// inline asm
	ld.u64 	%rd21, [%rd15+16];
	ld.u32 	%r64, [%rd21];

BB12_16:
	add.s32 	%r48, %r1, %r64;
	add.s32 	%r49, %r48, -1;
	ld.u32 	%r50, [%rd21+44];
	mul.lo.s32 	%r51, %r49, %r50;
	add.s32 	%r52, %r2, %r64;
	add.s32 	%r53, %r52, -1;
	ld.u32 	%r54, [%rd21+48];
	mad.lo.s32 	%r55, %r53, %r54, %r51;
	add.s32 	%r56, %r3, %r64;
	add.s32 	%r57, %r56, -1;
	ld.u32 	%r58, [%rd21+52];
	neg.s32 	%r59, %r58;
	mul.lo.s32 	%r60, %r57, %r59;
	setp.eq.s32	%p12, %r55, %r60;
	selp.u32	%r61, 1, 0, %p12;
	setp.ne.s32	%p13, %r61, %r20;
	@%p13 bra 	BB12_18;

	// inline asm
	trap;
	// inline asm

BB12_18:
	mov.u32 	%r62, 0;
	st.param.b32	[func_retval0+0], %r62;
	ret;
}

	// .globl	_Z10cs_stridesP8coords_sPiS1_S1_
.visible .func  (.param .b32 func_retval0) _Z10cs_stridesP8coords_sPiS1_S1_(
	.param .b64 _Z10cs_stridesP8coords_sPiS1_S1__param_0,
	.param .b64 _Z10cs_stridesP8coords_sPiS1_S1__param_1,
	.param .b64 _Z10cs_stridesP8coords_sPiS1_S1__param_2,
	.param .b64 _Z10cs_stridesP8coords_sPiS1_S1__param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [_Z10cs_stridesP8coords_sPiS1_S1__param_0];
	ld.param.u64 	%rd2, [_Z10cs_stridesP8coords_sPiS1_S1__param_1];
	ld.param.u64 	%rd3, [_Z10cs_stridesP8coords_sPiS1_S1__param_2];
	ld.param.u64 	%rd4, [_Z10cs_stridesP8coords_sPiS1_S1__param_3];
	setp.ne.s64	%p1, %rd1, 0;
	@%p1 bra 	BB13_2;

	// inline asm
	trap;
	// inline asm

BB13_2:
	ld.u64 	%rd5, [%rd1+16];
	ld.u32 	%r1, [%rd5+44];
	st.u32 	[%rd2], %r1;
	ld.u64 	%rd6, [%rd1+16];
	ld.u32 	%r2, [%rd6+48];
	st.u32 	[%rd3], %r2;
	ld.u64 	%rd7, [%rd1+16];
	ld.u32 	%r3, [%rd7+52];
	st.u32 	[%rd4], %r3;
	mov.u32 	%r4, 0;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	// .globl	_Z7cs_nallP8coords_sPi
.visible .func  (.param .b32 func_retval0) _Z7cs_nallP8coords_sPi(
	.param .b64 _Z7cs_nallP8coords_sPi_param_0,
	.param .b64 _Z7cs_nallP8coords_sPi_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [_Z7cs_nallP8coords_sPi_param_0];
	ld.param.u64 	%rd2, [_Z7cs_nallP8coords_sPi_param_1];
	setp.ne.s64	%p1, %rd1, 0;
	@%p1 bra 	BB14_2;

	// inline asm
	trap;
	// inline asm

BB14_2:
	ld.u64 	%rd3, [%rd1+16];
	ld.u32 	%r1, [%rd3];
	shl.b32 	%r2, %r1, 1;
	ld.u32 	%r3, [%rd3+20];
	add.s32 	%r4, %r2, %r3;
	st.u32 	[%rd2], %r4;
	ld.u64 	%rd4, [%rd1+16];
	ld.u32 	%r5, [%rd4];
	shl.b32 	%r6, %r5, 1;
	ld.u32 	%r7, [%rd4+24];
	add.s32 	%r8, %r6, %r7;
	st.u32 	[%rd2+4], %r8;
	ld.u64 	%rd5, [%rd1+16];
	ld.u32 	%r9, [%rd5];
	shl.b32 	%r10, %r9, 1;
	ld.u32 	%r11, [%rd5+28];
	add.s32 	%r12, %r10, %r11;
	st.u32 	[%rd2+8], %r12;
	mov.u32 	%r13, 0;
	st.param.b32	[func_retval0+0], %r13;
	ret;
}


